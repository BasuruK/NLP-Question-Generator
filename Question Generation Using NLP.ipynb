{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0b6187b510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "#Import PyTorch Framework\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = CRFTagger()\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the train set data\n",
    "#train_set  = pd.read_csv('Train_and_test_data/train.txt',sep=' ',names=['word','Brill','tag']).drop('Brill',1)\n",
    "# Format train set data to tuples of a list of lists\n",
    "#train_set = [[tuple(x) for x in train_set.values]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once on a time there was a Little Old Woman who lived in a Shoe. This shoe stood near a great forest, and was so large that it served as a house for the Old Lady and all her children, of which she had so many that she did not know what to do with them.\\n \\n But the Little Old Woman was very fond of her children, and they only thought of the best way to please her. Strong-arm, the eldest, cut down trees for firewood. Peter made baskets of wicker-work. Mark was chief gardener. Lizzie milked the cow, and Jenny taught the younger children to read.\\n \\n Now this Little Old Woman had not always lived in a Shoe. She and her family had once dwelt in a nice house covered with ivy, and her husband was a wood-cutter, like Strong-arm. But there lived in a huge castle beyond the forest, a fierce giant, who one day came and laid their house in ruins with his club; after which he carried off the poor wood-cutter to his castle beyond the forest. When the Little Old Woman came home, her house was in ruins and her husband was no where to be seen.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Code, Keep it commented unless retraining\n",
    "#ct.train(train_set,'model.crf.tagger') \n",
    "\n",
    "#sentence = \"Once upon a time there was a little girl called Cinderella. Cinderella is met a fairy who grants her a wish to be a Princess. Fairy said to Cinderella that she will become a Princess for one night\"\n",
    "#sentence = \"Once upon a time there was a little girl called Alice . One day Alice met a Rabbit . The Rabbit said to Alice , follow me Alice I will carry you to my Den\"\n",
    "\n",
    "# Read the Test data from txt\n",
    "sentence = ' '.join(open('Text_Passages2.txt','r').readlines()).rstrip(\"\\n\")\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(passage):\n",
    "    \"\"\"\n",
    "    1.Remove Punctuations and Special characters appear in book chapters\n",
    "    2.Remove Stopwords\n",
    "    3.Return Clean list of words\n",
    "    \"\"\"\n",
    "    exclude_set = set(['“','”',':'])\n",
    "\n",
    "    no_punctuation = [char for char in passage if char not in string.punctuation + \"“”.\"]\n",
    "    no_punctuation = ''.join(no_punctuation)\n",
    "    no_punctuation = [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time Little Old Woman lived Shoe shoe stood near great forest large served house Old Lady children many know Little Old Woman fond children thought best way please Strongarm eldest cut trees firewood Peter made baskets wickerwork Mark chief gardener Lizzie milked cow Jenny taught younger children read Little Old Woman always lived Shoe family dwelt nice house covered ivy husband woodcutter like Strongarm lived huge castle beyond forest fierce giant one day came laid house ruins club carried poor woodcutter castle beyond forest Little Old Woman came home house ruins husband seen\n"
     ]
    }
   ],
   "source": [
    "#remove stop words and punctuation\n",
    "sentence = process_text(sentence)\n",
    "sentence = ' '.join(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(sentence)\n",
    "word_list = [[]]\n",
    "word_list.clear()\n",
    "wlist = []\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    for word in sent.split():\n",
    "        wlist.append(word)\n",
    "        j += 1\n",
    "        \n",
    "    wlist.copy\n",
    "    word_list.append(wlist.copy())\n",
    "    wlist.clear()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set the model of previously trained data set\n",
    "ct.set_model_file(model_file='model.crf.tagger')\n",
    "word_list = ct.tag_sents(word_list)\n",
    "\n",
    "#test_set = pd.read_csv('Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag'], index_col=False).drop(['Brill','tag'],1)\n",
    "#test_set = test_set.values.tolist()\n",
    "#ct.tag_sents(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_setEval = pd.read_csv('Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag']).drop('Brill',1)\n",
    "#test_setEval = [[tuple(x) for x in test_setEval.values]]\n",
    "#test the accuracy of the POS\n",
    "#ct.evaluate(test_setEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['time', 'B-NP'],\n",
       "       ['Little', 'I-NP'],\n",
       "       ['Old', 'I-NP'],\n",
       "       ['Woman', 'I-NP'],\n",
       "       ['lived', 'B-VP'],\n",
       "       ['Shoe', 'B-NP'],\n",
       "       ['shoe', 'I-NP'],\n",
       "       ['stood', 'I-NP'],\n",
       "       ['near', 'B-PP'],\n",
       "       ['great', 'B-NP'],\n",
       "       ['forest', 'I-NP'],\n",
       "       ['large', 'I-NP'],\n",
       "       ['served', 'B-VP'],\n",
       "       ['house', 'I-VP'],\n",
       "       ['Old', 'B-NP'],\n",
       "       ['Lady', 'I-NP'],\n",
       "       ['children', 'I-NP'],\n",
       "       ['many', 'I-NP'],\n",
       "       ['know', 'B-VP'],\n",
       "       ['Little', 'B-NP'],\n",
       "       ['Old', 'I-NP'],\n",
       "       ['Woman', 'I-NP'],\n",
       "       ['fond', 'I-NP'],\n",
       "       ['children', 'I-NP'],\n",
       "       ['thought', 'I-NP'],\n",
       "       ['best', 'I-NP'],\n",
       "       ['way', 'I-NP'],\n",
       "       ['please', 'I-NP'],\n",
       "       ['Strongarm', 'I-NP'],\n",
       "       ['eldest', 'I-NP'],\n",
       "       ['cut', 'I-NP'],\n",
       "       ['trees', 'I-NP'],\n",
       "       ['firewood', 'I-NP'],\n",
       "       ['Peter', 'I-NP'],\n",
       "       ['made', 'B-VP'],\n",
       "       ['baskets', 'B-NP'],\n",
       "       ['wickerwork', 'I-NP'],\n",
       "       ['Mark', 'I-NP'],\n",
       "       ['chief', 'I-NP'],\n",
       "       ['gardener', 'I-NP'],\n",
       "       ['Lizzie', 'I-NP'],\n",
       "       ['milked', 'B-VP'],\n",
       "       ['cow', 'I-VP'],\n",
       "       ['Jenny', 'B-NP'],\n",
       "       ['taught', 'I-NP'],\n",
       "       ['younger', 'I-NP'],\n",
       "       ['children', 'I-NP'],\n",
       "       ['read', 'I-NP'],\n",
       "       ['Little', 'I-NP'],\n",
       "       ['Old', 'I-NP'],\n",
       "       ['Woman', 'I-NP'],\n",
       "       ['always', 'B-VP'],\n",
       "       ['lived', 'I-VP'],\n",
       "       ['Shoe', 'B-NP'],\n",
       "       ['family', 'I-NP'],\n",
       "       ['dwelt', 'I-NP'],\n",
       "       ['nice', 'I-NP'],\n",
       "       ['house', 'I-NP'],\n",
       "       ['covered', 'B-VP'],\n",
       "       ['ivy', 'B-NP'],\n",
       "       ['husband', 'I-NP'],\n",
       "       ['woodcutter', 'I-NP'],\n",
       "       ['like', 'B-PP'],\n",
       "       ['Strongarm', 'B-NP'],\n",
       "       ['lived', 'B-VP'],\n",
       "       ['huge', 'B-NP'],\n",
       "       ['castle', 'I-NP'],\n",
       "       ['beyond', 'I-NP'],\n",
       "       ['forest', 'I-NP'],\n",
       "       ['fierce', 'I-NP'],\n",
       "       ['giant', 'I-NP'],\n",
       "       ['one', 'I-NP'],\n",
       "       ['day', 'I-NP'],\n",
       "       ['came', 'B-VP'],\n",
       "       ['laid', 'I-VP'],\n",
       "       ['house', 'I-VP'],\n",
       "       ['ruins', 'B-NP'],\n",
       "       ['club', 'I-NP'],\n",
       "       ['carried', 'I-NP'],\n",
       "       ['poor', 'I-NP'],\n",
       "       ['woodcutter', 'I-NP'],\n",
       "       ['castle', 'I-NP'],\n",
       "       ['beyond', 'I-NP'],\n",
       "       ['forest', 'I-NP'],\n",
       "       ['Little', 'I-NP'],\n",
       "       ['Old', 'I-NP'],\n",
       "       ['Woman', 'I-NP'],\n",
       "       ['came', 'B-VP'],\n",
       "       ['home', 'B-NP'],\n",
       "       ['house', 'I-NP'],\n",
       "       ['ruins', 'B-VP'],\n",
       "       ['husband', 'I-VP'],\n",
       "       ['seen', 'I-VP']], \n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert list of tuples to a numpy array\n",
    "word_list = np.array(word_list)\n",
    "word_list = np.reshape(word_list, (-1,2))\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find S -> NP VP NP\n",
    "# Returns if a subject is found\n",
    "# pattern variable = [pat0, pat1, pat2]\n",
    "def findSubject(pattern):\n",
    "    #print('{} -> {} {}'.format(pattern[0], pattern[1], pattern[2]))\n",
    "    if pattern[0] == 'NP' and pattern[1] == 'VP' and pattern[2] == 'NP':\n",
    "        #if true pattern[0] is the subject of a sentence\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Subject Found at index 3\n",
      "Subject -> Woman => lived Shoe\n",
      "--------------------\n",
      "Potential Subject Found at index 17\n",
      "Subject -> many => know Little\n",
      "--------------------\n",
      "Potential Subject Found at index 33\n",
      "Subject -> Peter => made baskets\n",
      "--------------------\n",
      "Potential Subject Found at index 57\n",
      "Subject -> house => covered ivy\n",
      "--------------------\n",
      "Potential Subject Found at index 63\n",
      "Subject -> Strongarm => lived huge\n",
      "--------------------\n",
      "Potential Subject Found at index 86\n",
      "Subject -> Woman => came home\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#Extract the Subject -> NP VP NP to find the subjects in a sentence\n",
    "#using Markov Chain Model\n",
    "\n",
    "pattern = []\n",
    "subjects = []\n",
    "word_list_length = len(word_list)\n",
    "\n",
    "for i in range(0, len(word_list)):\n",
    "    #print(word_list[i][1].split('-')[1])\n",
    "    try:\n",
    "        if((word_list_length - i == 2)):\n",
    "            break\n",
    "        else:\n",
    "            pattern = [word_list[i][1].split('-')[1], word_list[i+1][1].split('-')[1], word_list[i+2][1].split('-')[1]]\n",
    "            # Call the function findSubject to identify potential subject elements and save them in an array\n",
    "            if findSubject(pattern):\n",
    "                # If returns true consider 1st element as a potential Subject\n",
    "                print('Potential Subject Found at index {}'.format(i))\n",
    "                print('Subject -> {} => {} {}'.format(word_list[i][0], word_list[i+1][0], word_list[i+2][0]))\n",
    "                # Put the phrases in to a sentece and append it to an array.\n",
    "                sub = [word_list[i][0],word_list[i+1][0],word_list[i+2][0]]\n",
    "                subjects.append(' '.join(sub))\n",
    "                print('--------------------')\n",
    "            \n",
    "    except IndexError:\n",
    "        # Break the loop if IndexError occurs\n",
    "        # Fail safe\n",
    "        print(\"Out of Index\")\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_POS-TAG_Person</th>\n",
       "      <th>nullColumn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woman_NP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lived_VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shoe_NP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many_NP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>know_VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Little_NP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Peter_SBAR_PERSON</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>made_VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>baskets_VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>house_SBAR</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>covered_VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ivy_ADVP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Strongarm_NP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lived_VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>huge_NP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Woman_NP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>came_VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>home_ADVP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word_POS-TAG_Person nullColumn\n",
       "0             Woman_NP           \n",
       "1             lived_VP           \n",
       "2              Shoe_NP           \n",
       "3              many_NP           \n",
       "4              know_VP           \n",
       "5            Little_NP           \n",
       "6    Peter_SBAR_PERSON           \n",
       "7              made_VP           \n",
       "8           baskets_VP           \n",
       "9           house_SBAR           \n",
       "10          covered_VP           \n",
       "11            ivy_ADVP           \n",
       "12        Strongarm_NP           \n",
       "13            lived_VP           \n",
       "14             huge_NP           \n",
       "15            Woman_NP           \n",
       "16             came_VP           \n",
       "17           home_ADVP           "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find PERSONs in the filtered sentence using spaCy NER and build a DataFrame Object based on the data\n",
    "# DataFrame structure -> Word | POS_TAG | Person\n",
    "\n",
    "#Convert the subjects array to a 2D numpy array\n",
    "subjects = np.array(subjects)\n",
    "subjects = np.reshape(subjects, (-1,1))\n",
    "\n",
    "refferingDataFrame = pd.DataFrame(columns=('Word_POS-TAG_Person','nullColumn'))\n",
    "\n",
    "for i in range(0,len(subjects)):\n",
    "    for j in range(0,len(subjects[i][0].split(\" \"))):\n",
    "        wordBag = subjects[i][0].split(\" \")\n",
    "        # Find POS_TAG \n",
    "        pos = ct.tag([wordBag[j]])\n",
    "        \n",
    "        # Find IF word is Person\n",
    "        person = [ent.label_ for ent in nlp(pos[0][0]).ents]\n",
    "        person = ' '.join(person)\n",
    "        if not person:\n",
    "            person = \"\"\n",
    "            \n",
    "        #Add a row to the DataFrame with the retrived data\n",
    "        refferingDataFrame.loc[len(refferingDataFrame)] = [(pos[0][0] +\"_\"+ pos[0][1].split('-')[1] +\"_\"+ person).rstrip('_'), \"\"]\n",
    "        \n",
    "refferingDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create the LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 25.0% Completed\n",
      "Training 50.0% Completed\n",
      "Training 75.0% Completed\n",
      "Training Complete!\n",
      "Total Training time : 23.8 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The RNN is created using PyTorch Framework with Cuda disabled. The RNN will clasify the Question according the the catogiry\n",
    "# To enable cuda add .cuda() method to torch.randn() method\n",
    "lstm = nn.LSTM(3, 3)\n",
    "inputs = [autograd.Variable(torch.randn(1, 3)) for _ in range(5)]\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn(1, 1, 3)))\n",
    "\n",
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    \n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn(1, 1, 3)))\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "#print(out)\n",
    "#print(hidden)\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "# Read training data from the csv\n",
    "# Training data format => [([\"Phrase\"], [\"Question\"])]\n",
    "unprocessed_data = pd.read_csv('Train_and_test_data/LSTM_train_set.csv',header=None)\n",
    "\n",
    "# Transform the data in to a processable format\n",
    "t_data_for_lstm = []\n",
    "training_data = []\n",
    "for phrase in unprocessed_data.itertuples():\n",
    "    t_data_for_lstm.append(list(zip([[phrase[1]]], [[phrase[2]]])))\n",
    "\n",
    "for i in range(0, len(t_data_for_lstm)):\n",
    "    training_data.append(t_data_for_lstm[i][0])\n",
    "    \n",
    "word_to_ix = {}\n",
    "tag_to_ix = {}\n",
    "\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "    for word1 in tags:\n",
    "        if word1 not in tag_to_ix:\n",
    "            tag_to_ix[word1] = len(tag_to_ix)\n",
    "                            \n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# Create the model\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden  = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores\n",
    "    \n",
    "# set the variables to Train the model\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "#Values Before Training\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "t0 = time.time()\n",
    "iterations = 300\n",
    "progress_after = iterations/4\n",
    "\n",
    "# Train the RNN for 300 iterations\n",
    "for epoch in range(0,iterations):\n",
    "    for sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        # Calculate the % of completion\n",
    "        if(progress_after == epoch):\n",
    "            print(\"Training {}% Completed\".format((progress_after / iterations) * 100))\n",
    "            progress_after = progress_after + (iterations / 4)\n",
    "            \n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print(\"Training Complete!\\nTotal Training time :\", round(time.time()-t0, 2), \"s\\n\")\n",
    "\n",
    "# #Values after training        \n",
    "# inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "# tag_scores = model(inputs)\n",
    "\n",
    "# print(\"Values After training\")\n",
    "# print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Format the result generated by the LSTM by adding the relelevent Noun Phrases\n",
    "def generate_questions(model, ref_data_frame):\n",
    "    question = \"\"\n",
    "    sentence = \"\"\n",
    "    index = 0\n",
    "    # for all the entries in the ref_data_frame predict a question\n",
    "    for index, word_phrase in ref_data_frame.iterrows():\n",
    "        \n",
    "        # Predict the potentian sentence structure that can be used to generate the question.\n",
    "        try:\n",
    "            inputs = prepare_sequence([word_phrase[0]], word_to_ix)\n",
    "            tag_scores = model(inputs)\n",
    "        except KeyError:\n",
    "            # ignore\n",
    "            pass\n",
    "\n",
    "        # Take the maximum probalilty, and based on the probabilty find index of value of the dictonary\n",
    "        maxVal = max(tag_scores.data.numpy()[0])\n",
    "        index_loc = 0\n",
    "        probability_tag_scores = tag_scores.data.numpy().ravel()\n",
    "\n",
    "        for i in range(0, len(probability_tag_scores)):\n",
    "            if (probability_tag_scores.ravel()[i] == maxVal):\n",
    "                index_loc = i\n",
    "\n",
    "        # Travers the dictonary and identify the key value based on the probability predicted\n",
    "        for key, value in tag_to_ix.items():\n",
    "            if (value == index_loc):\n",
    "                # format the Key to generate a meaningfull question\n",
    "                # extract subject and object of the tested sentence\n",
    "                sentence = word_phrase[0].split('_')\n",
    "                # Check whether the question needs modification\n",
    "                if \"NP\" in sentence:\n",
    "                    question = key.replace(\"NP\", sentence[0])\n",
    "                    print(question)\n",
    "                elif \"VP\" in sentence:\n",
    "                    if \"NP\" in key and \"N1\" in key:\n",
    "                        # Find the complete sentence matching for the verb\n",
    "                        # Find the index in subjects which matches for verb\n",
    "                        index = [i for i,j in enumerate(subjects.ravel()) if sentence[0] in j]\n",
    "                        ref_subject = subjects.ravel()[index][0].split()\n",
    "                        # Replace for words NP and NP1\n",
    "                        question = key.replace(\"NP\", ref_subject[0]) \n",
    "                        question = question.replace(\"N1\",ref_subject[-1])\n",
    "                        print(question)\n",
    "                        \n",
    "                    else:\n",
    "                        # If \"VP\" but only one \"NP\"\n",
    "                        index = [i for i,j in enumerate(subjects.ravel()) if sentence[0] in j]\n",
    "                        ref_subject = subjects.ravel()[index][0].split()\n",
    "                        question = key.replace(\"NP\", ref_subject[0])\n",
    "                        print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the Woman ?\n",
      "Where did the Woman live ?\n",
      "Where did the Shoe live ?\n",
      "Where did the many live ?\n",
      "Where did the many live ?\n",
      "Who is the Little ?\n",
      "What did Peter make ?\n",
      "Who is Peter ?\n",
      "What was the house ?\n",
      "What was the Strongarm ?\n",
      "Where did the Woman live ?\n",
      "What was huge ?\n",
      "Who is the Woman ?\n",
      "Where did the Woman live ?\n"
     ]
    }
   ],
   "source": [
    "generate_questions(model, refferingDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
