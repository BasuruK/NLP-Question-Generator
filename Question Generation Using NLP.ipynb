{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4ee83af510>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "#Import PyTorch Framework\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = CRFTagger()\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the train set data\n",
    "#train_set  = pd.read_csv('Train_and_test_data/train.txt',sep=' ',names=['word','Brill','tag']).drop('Brill',1)\n",
    "# Format train set data to tuples of a list of lists\n",
    "#train_set = [[tuple(x) for x in train_set.values]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training Code, Keep it commented unless retraining\n",
    "#ct.train(train_set,'model.crf.tagger') \n",
    "\n",
    "#sentence = \"Once upon a time there was a little girl called Cinderella. Cinderella is met a fairy who grants her a wish to be a Princess. Fairy said to Cinderella that she will become a Princess for one night\"\n",
    "sentence = \"Once upon a time there was a little girl called Alice . One day Alice met a Rabbit . The Rabbit said to Alice , follow me Alice I will carry you to my Den\"\n",
    "#sentence = \"In a galaxy far far away. there was a girl who had powers of a jedi named Kathrine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(passage):\n",
    "    \"\"\"\n",
    "    1.Remove Punctuations and Special characters appear in book chapters\n",
    "    2.Remove Stopwords\n",
    "    3.Return Clean list of words\n",
    "    \"\"\"\n",
    "    exclude_set = set(['“','”',':'])\n",
    "\n",
    "    no_punctuation = [char for char in passage if char not in string.punctuation + \"“”.\"]\n",
    "    no_punctuation = ''.join(no_punctuation)\n",
    "    no_punctuation = [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a little girl called Alice . One day Alice met a Rabbit . The Rabbit said to Alice , follow me Alice I will carry you to my Den\n"
     ]
    }
   ],
   "source": [
    "#remove stop words and punctuation\n",
    "sentence = process_text(sentence)\n",
    "sentence = ' '.join(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Once', 'upon', 'a', 'time', 'there', 'was', 'a', 'little', 'girl', 'called', 'Alice', '.'], ['One', 'day', 'Alice', 'met', 'a', 'Rabbit', '.'], ['The', 'Rabbit', 'said', 'to', 'Alice', ',', 'follow', 'me', 'Alice', 'I', 'will', 'carry', 'you', 'to', 'my', 'Den']]\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(sentence)\n",
    "word_list = [[]]\n",
    "word_list.clear()\n",
    "wlist = []\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    for word in sent.split():\n",
    "        wlist.append(word)\n",
    "        j += 1\n",
    "        \n",
    "    wlist.copy\n",
    "    word_list.append(wlist.copy())\n",
    "    wlist.clear()\n",
    "    i += 1\n",
    "    \n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set the model of previously trained data set\n",
    "ct.set_model_file(model_file='model.crf.tagger')\n",
    "word_list = ct.tag_sents(word_list)\n",
    "\n",
    "#test_set = pd.read_csv('Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag'], index_col=False).drop(['Brill','tag'],1)\n",
    "#test_set = test_set.values.tolist()\n",
    "#ct.tag_sents(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_setEval = pd.read_csv('Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag']).drop('Brill',1)\n",
    "#test_setEval = [[tuple(x) for x in test_setEval.values]]\n",
    "#test the accuracy of the POS\n",
    "#ct.evaluate(test_setEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Once', 'B-NP'],\n",
       "       ['upon', 'B-PP'],\n",
       "       ['a', 'B-NP'],\n",
       "       ['time', 'I-NP'],\n",
       "       ['there', 'B-NP'],\n",
       "       ['was', 'B-VP'],\n",
       "       ['a', 'B-NP'],\n",
       "       ['little', 'I-NP'],\n",
       "       ['girl', 'I-NP'],\n",
       "       ['called', 'B-VP'],\n",
       "       ['Alice', 'B-NP'],\n",
       "       ['.', 'O'],\n",
       "       ['One', 'B-NP'],\n",
       "       ['day', 'I-NP'],\n",
       "       ['Alice', 'I-NP'],\n",
       "       ['met', 'B-VP'],\n",
       "       ['a', 'B-NP'],\n",
       "       ['Rabbit', 'I-NP'],\n",
       "       ['.', 'O'],\n",
       "       ['The', 'B-NP'],\n",
       "       ['Rabbit', 'I-NP'],\n",
       "       ['said', 'B-VP'],\n",
       "       ['to', 'B-PP'],\n",
       "       ['Alice', 'B-NP'],\n",
       "       [',', 'O'],\n",
       "       ['follow', 'B-VP'],\n",
       "       ['me', 'B-NP'],\n",
       "       ['Alice', 'I-NP'],\n",
       "       ['I', 'B-NP'],\n",
       "       ['will', 'B-VP'],\n",
       "       ['carry', 'I-VP'],\n",
       "       ['you', 'B-NP'],\n",
       "       ['to', 'B-PP'],\n",
       "       ['my', 'B-NP'],\n",
       "       ['Den', 'I-NP']], \n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert list of tuples to a numpy array\n",
    "word_list = np.array(word_list)\n",
    "word_list = np.reshape(word_list, (-1,2))\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find S -> NP VP NP\n",
    "# Returns if a subject is found\n",
    "# pattern variable = [pat0, pat1, pat2]\n",
    "def findSubject(pattern):\n",
    "    #print('{} -> {} {}'.format(pattern[0], pattern[1], pattern[2]))\n",
    "    if pattern[0] == 'NP' and pattern[1] == 'VP' and pattern[2] == 'NP':\n",
    "        #if true pattern[0] is the subject of a sentence\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Subject Found at index 4\n",
      "Subject -> there => was a\n",
      "--------------------\n",
      "Potential Subject Found at index 8\n",
      "Subject -> girl => called Alice\n",
      "--------------------\n",
      "Out of Index\n"
     ]
    }
   ],
   "source": [
    "#Extract the Subject -> NP VP NP to find the subjects in a sentence\n",
    "#using Markov Chain Model\n",
    "\n",
    "pattern = []\n",
    "subjects = []\n",
    "word_list_length = len(word_list)\n",
    "\n",
    "for i in range(0, len(word_list)):\n",
    "    #print(word_list[i][1].split('-')[1])\n",
    "    try:\n",
    "        if((word_list_length - i == 2)):\n",
    "            break\n",
    "        else:\n",
    "            pattern = [word_list[i][1].split('-')[1], word_list[i+1][1].split('-')[1], word_list[i+2][1].split('-')[1]]\n",
    "            # Call the function findSubject to identify potential subject elements and save them in an array\n",
    "            if findSubject(pattern):\n",
    "                # If returns true consider 1st element as a potential Subject\n",
    "                print('Potential Subject Found at index {}'.format(i))\n",
    "                print('Subject -> {} => {} {}'.format(word_list[i][0], word_list[i+1][0], word_list[i+2][0]))\n",
    "                # Put the phrases in to a sentece and append it to an array.\n",
    "                sub = [word_list[i][0],word_list[i+1][0],word_list[i+2][0]]\n",
    "                subjects.append(' '.join(sub))\n",
    "                print('--------------------')\n",
    "            \n",
    "    except IndexError:\n",
    "        # Break the loop if IndexError occurs\n",
    "        # Fail safe\n",
    "        print(\"Out of Index\")\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_POS-TAG_Person</th>\n",
       "      <th>nullColumn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there_ADVP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was_VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a_NP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>girl_ADJP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>called_VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alice_NP_PERSON</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Word_POS-TAG_Person nullColumn\n",
       "0          there_ADVP           \n",
       "1              was_VP           \n",
       "2                a_NP           \n",
       "3           girl_ADJP           \n",
       "4           called_VP           \n",
       "5     Alice_NP_PERSON           "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find PERSONs in the filtered sentence using spaCy NER and build a DataFrame Object based on the data\n",
    "# DataFrame structure -> Word | POS_TAG | Person\n",
    "\n",
    "#Convert the subjects array to a 2D numpy array\n",
    "subjects = np.array(subjects)\n",
    "subjects = np.reshape(subjects, (-1,1))\n",
    "\n",
    "refferingDataFrame = pd.DataFrame(columns=('Word_POS-TAG_Person','nullColumn'))\n",
    "\n",
    "for i in range(0,len(subjects)):\n",
    "    for j in range(0,len(subjects[i][0].split(\" \"))):\n",
    "        wordBag = subjects[i][0].split(\" \")\n",
    "        # Find POS_TAG \n",
    "        pos = ct.tag([wordBag[j]])\n",
    "        \n",
    "        # Find IF word is Person\n",
    "        person = [ent.label_ for ent in nlp(pos[0][0]).ents]\n",
    "        person = ' '.join(person)\n",
    "        if not person:\n",
    "            person = \"\"\n",
    "            \n",
    "        #Add a row to the DataFrame with the retrived data\n",
    "        refferingDataFrame.loc[len(refferingDataFrame)] = [(pos[0][0] +\"_\"+ pos[0][1].split('-')[1] +\"_\"+ person).rstrip('_'), \"\"]\n",
    "        \n",
    "refferingDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create the LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time :  14.71 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The RNN is created using PyTorch Framework with Cuda disabled. The RNN will clasify the Question according the the catogiry\n",
    "# To enable cuda add .cuda() method to torch.randn() method\n",
    "lstm = nn.LSTM(3, 3)\n",
    "inputs = [autograd.Variable(torch.randn(1, 3)) for _ in range(5)]\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn(1, 1, 3)))\n",
    "\n",
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    \n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn(1, 1, 3)))\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "#print(out)\n",
    "#print(hidden)\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "# Read training data from the csv\n",
    "# Training data format => [([\"Phrase\"], [\"Question\"])]\n",
    "unprocessed_data = pd.read_csv('Train_and_test_data/LSTM_train_set.csv',header=None)\n",
    "\n",
    "# Transform the data in to a processable format\n",
    "t_data_for_lstm = []\n",
    "training_data = []\n",
    "for phrase in unprocessed_data.itertuples():\n",
    "    t_data_for_lstm.append(list(zip([[phrase[1]]], [[phrase[2]]])))\n",
    "\n",
    "for i in range(0, len(t_data_for_lstm)):\n",
    "    training_data.append(t_data_for_lstm[i][0])\n",
    "    \n",
    "word_to_ix = {}\n",
    "tag_to_ix = {}\n",
    "\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "    for word1 in tags:\n",
    "        if word1 not in tag_to_ix:\n",
    "            tag_to_ix[word1] = len(tag_to_ix)\n",
    "                            \n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# Create the model\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden  = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores\n",
    "    \n",
    "# set the variables to Train the model\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "#Values Before Training\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "t0 = time.time()\n",
    "# Train the RNN for 300 iterations\n",
    "for epoch in range(300):\n",
    "    for sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "print(\"Training time : \", round(time.time()-t0, 2), \"s\\n\")\n",
    "\n",
    "# #Values after training        \n",
    "# inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "# tag_scores = model(inputs)\n",
    "\n",
    "# print(\"Values After training\")\n",
    "# print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_questions(model, ref_data_frame):\n",
    "    question = \"\"\n",
    "    sentence = \"\"\n",
    "    index = 0\n",
    "    # for all the entries in the ref_data_frame predict a question\n",
    "    for index, word_phrase in ref_data_frame.iterrows():\n",
    "        \n",
    "        # Predict the potentian sentence structure that can be used to generate the question.\n",
    "        inputs = prepare_sequence([word_phrase[0]], word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "\n",
    "        # Take the maximum probalilty, and based on the probabilty find index of value of the dictonary\n",
    "        maxVal = max(tag_scores.data.numpy()[0])\n",
    "        index_loc = 0\n",
    "        probability_tag_scores = tag_scores.data.numpy().ravel()\n",
    "\n",
    "        for i in range(0, len(probability_tag_scores)):\n",
    "            if (probability_tag_scores.ravel()[i] == maxVal):\n",
    "                index_loc = i\n",
    "\n",
    "        # Travers the dictonary and identify the key value based on the probability predicted\n",
    "        for key, value in tag_to_ix.items():\n",
    "            if (value == index_loc):\n",
    "                # format the Key to generate a meaningfull question\n",
    "                # extract subject and object of the tested sentence\n",
    "                sentence = word_phrase[0].split('_')\n",
    "                # Check whether the question needs modification\n",
    "                if \"NP\" in sentence:\n",
    "                    question = key.replace(\"NP\", sentence[0])\n",
    "                    print(question)\n",
    "                elif \"VP\" in sentence:\n",
    "                    if \"NP\" in key and \"N1\" in key:\n",
    "                        # Find the complete sentence matching for the verb\n",
    "                        # Find the index in subjects which matches for verb\n",
    "                        index = [i for i,j in enumerate(subjects.ravel()) if sentence[0] in j]\n",
    "                        ref_subject = subjects.ravel()[index][0].split()\n",
    "                        # Replace for words NP and NP1\n",
    "                        question = key.replace(\"NP\", ref_subject[0]) \n",
    "                        question = question.replace(\"N1\",ref_subject[-1])\n",
    "                        print(question)\n",
    "                        \n",
    "                    else:\n",
    "                        # If \"VP\" but only one \"NP\"\n",
    "                        index = [i for i,j in enumerate(subjects.ravel()) if sentence[0] in j]\n",
    "                        ref_subject = subjects.ravel()[index][0].split()\n",
    "                        question = key.replace(\"NP\", ref_subject[0])\n",
    "                        print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'there_ADVP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-8cea660bc513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_questions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefferingDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-175-8d20025a06f2>\u001b[0m in \u001b[0;36mgenerate_questions\u001b[0;34m(model, ref_data_frame)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Predict the potentian sentence structure that can be used to generate the question.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_phrase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-ec1ff1df3a94>\u001b[0m in \u001b[0;36mprepare_sequence\u001b[0;34m(seq, to_ix)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-ec1ff1df3a94>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'there_ADVP'"
     ]
    }
   ],
   "source": [
    "generate_questions(model, refferingDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
