{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe03f2cb510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "#Import PyTorch Framework\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = CRFTagger()\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the train set data\n",
    "#train_set  = pd.read_csv('Train_and_test_data/train.txt',sep=' ',names=['word','Brill','tag']).drop('Brill',1)\n",
    "# Format train set data to tuples of a list of lists\n",
    "#train_set = [[tuple(x) for x in train_set.values]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training Code, Keep it commented unless retraining\n",
    "#ct.train(train_set,'model.crf.tagger') \n",
    "\n",
    "#sentence = \"Once upon a time there was a little girl called Cinderella. Cinderella is met a fairy who grants her a wish to be a Princess. Fairy said to Cinderella that she will become a Princess for one night\"\n",
    "sentence = \"Once upon a time there was a little girl called Alice . One day Alice met a Rabbit . The Rabbit said to Alice , follow me Alice I will carry you to my Den\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(passage):\n",
    "    \"\"\"\n",
    "    1.Remove Punctuations and Special characters appear in book chapters\n",
    "    2.Remove Stopwords\n",
    "    3.Return Clean list of words\n",
    "    \"\"\"\n",
    "    exclude_set = set(['“','”',':'])\n",
    "\n",
    "    no_punctuation = [char for char in passage if char not in string.punctuation + \"“”.\"]\n",
    "    no_punctuation = ''.join(no_punctuation)\n",
    "    no_punctuation = [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upon time little girl called Alice One day Alice met Rabbit Rabbit said Alice follow Alice carry Den\n"
     ]
    }
   ],
   "source": [
    "#remove stop words and punctuation\n",
    "sentence = process_text(sentence)\n",
    "sentence = ' '.join(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['upon', 'time', 'little', 'girl', 'called', 'Alice', 'One', 'day', 'Alice', 'met', 'Rabbit', 'Rabbit', 'said', 'Alice', 'follow', 'Alice', 'carry', 'Den']]\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(sentence)\n",
    "word_list = [[]]\n",
    "word_list.clear()\n",
    "wlist = []\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    for word in sent.split():\n",
    "        wlist.append(word)\n",
    "        j += 1\n",
    "        \n",
    "    wlist.copy\n",
    "    word_list.append(wlist.copy())\n",
    "    wlist.clear()\n",
    "    i += 1\n",
    "    \n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set the model of previously trained data set\n",
    "ct.set_model_file(model_file='model.crf.tagger')\n",
    "word_list = ct.tag_sents(word_list)\n",
    "\n",
    "#test_set = pd.read_csv('Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag'], index_col=False).drop(['Brill','tag'],1)\n",
    "#test_set = test_set.values.tolist()\n",
    "#ct.tag_sents(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_setEval = pd.read_csv('Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag']).drop('Brill',1)\n",
    "#test_setEval = [[tuple(x) for x in test_setEval.values]]\n",
    "#test the accuracy of the POS\n",
    "#ct.evaluate(test_setEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['upon', 'B-PP'],\n",
       "       ['time', 'B-NP'],\n",
       "       ['little', 'I-NP'],\n",
       "       ['girl', 'I-NP'],\n",
       "       ['called', 'B-VP'],\n",
       "       ['Alice', 'B-NP'],\n",
       "       ['One', 'I-NP'],\n",
       "       ['day', 'I-NP'],\n",
       "       ['Alice', 'I-NP'],\n",
       "       ['met', 'I-NP'],\n",
       "       ['Rabbit', 'I-NP'],\n",
       "       ['Rabbit', 'I-NP'],\n",
       "       ['said', 'B-VP'],\n",
       "       ['Alice', 'B-NP'],\n",
       "       ['follow', 'I-NP'],\n",
       "       ['Alice', 'I-NP'],\n",
       "       ['carry', 'I-NP'],\n",
       "       ['Den', 'I-NP']], \n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert list of tuples to a numpy array\n",
    "word_list = np.array(word_list)\n",
    "\n",
    "word_list = np.reshape(word_list, (-1,2))\n",
    "\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find S -> NP VP \n",
    "# Returns if a subject is found\n",
    "# pattern variable = [pat0, pat1, pat2]\n",
    "def findSubject(pattern):\n",
    "    #print('{} -> {} {}'.format(pattern[0], pattern[1], pattern[2]))\n",
    "    if pattern[0] == 'NP' and pattern[1] == 'VP' and pattern[2] == 'NP':\n",
    "        #if true pattern[0] is the subject of a sentence\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Subject Found at index 3\n",
      "Subject -> girl => called Alice\n",
      "--------------------\n",
      "Potential Subject Found at index 11\n",
      "Subject -> Rabbit => said Alice\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#Extract the Subject -> NP VP NP to find the subjects in a sentence\n",
    "#using Markov Chain Model\n",
    "\n",
    "pattern = []\n",
    "subjects = []\n",
    "word_list_length = len(word_list)\n",
    "\n",
    "for i in range(0, len(word_list)):\n",
    "    #print(word_list[i][1].split('-')[1])\n",
    "    try:\n",
    "        if((word_list_length - i == 2)):\n",
    "            break\n",
    "        else:\n",
    "            pattern = [word_list[i][1].split('-')[1], word_list[i+1][1].split('-')[1], word_list[i+2][1].split('-')[1]]\n",
    "            # Call the function findSubject to identify potential subject elements and save them in an array\n",
    "            if findSubject(pattern):\n",
    "                # If returns true consider 1st element as a potential Subject\n",
    "                print('Potential Subject Found at index {}'.format(i))\n",
    "                print('Subject -> {} => {} {}'.format(word_list[i][0], word_list[i+1][0], word_list[i+2][0]))\n",
    "                # Put the phrases in to a sentece and append it to an array.\n",
    "                sub = [word_list[i][0],word_list[i+1][0],word_list[i+2][0]]\n",
    "                subjects.append(' '.join(sub))\n",
    "                print('--------------------')\n",
    "            \n",
    "    except IndexError:\n",
    "        # Break the loop if IndexError occurs\n",
    "        # Fail safe\n",
    "        print(\"Out of Index\")\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS_TAG</th>\n",
       "      <th>Person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>girl</td>\n",
       "      <td>ADJP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>called</td>\n",
       "      <td>VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice</td>\n",
       "      <td>NP</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rabbit</td>\n",
       "      <td>NP</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>said</td>\n",
       "      <td>VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alice</td>\n",
       "      <td>NP</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word POS_TAG  Person\n",
       "0    girl    ADJP        \n",
       "1  called      VP        \n",
       "2   Alice      NP  PERSON\n",
       "3  Rabbit      NP  PERSON\n",
       "4    said      VP        \n",
       "5   Alice      NP  PERSON"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find PERSONs in the filtered sentence using spaCy NER and build a DataFrame Object based on the data\n",
    "# DataFrame structure -> Word | POS_TAG | Person\n",
    "\n",
    "#Convert the subjects array to a 2D numpy array\n",
    "subjects = np.array(subjects)\n",
    "subjects = np.reshape(subjects, (-1,1))\n",
    "\n",
    "refferingDataFrame = pd.DataFrame(columns=('Word','POS_TAG','Person'))\n",
    "\n",
    "for i in range(0,len(subjects)):\n",
    "    for j in range(0,len(subjects[i][0].split(\" \"))):\n",
    "        wordBag = subjects[i][0].split(\" \")\n",
    "        # Find POS_TAG \n",
    "        pos = ct.tag([wordBag[j]])\n",
    "        \n",
    "        # Find IF word is Person\n",
    "        person = [ent.label_ for ent in nlp(pos[0][0]).ents]\n",
    "        person = ' '.join(person)\n",
    "        if not person:\n",
    "            person = \"\"\n",
    "            \n",
    "        #Add a row to the DataFrame with the retrived data\n",
    "        refferingDataFrame.loc[len(refferingDataFrame)] = [pos[0][0], pos[0][1].split('-')[1], person]\n",
    "        \n",
    "refferingDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create the LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values before training\n",
      "Variable containing:\n",
      "-0.6587 -0.7288\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "=======================\n",
      "Values After training\n",
      "Variable containing:\n",
      "-0.0967 -2.3837\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)\n",
    "inputs = [autograd.Variable(torch.randn(1, 3)) for _ in range(5)]\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn(1, 1, 3)))\n",
    "\n",
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    \n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)), autograd.Variable(torch.randn(1, 1, 3)))\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "#print(out)\n",
    "#print(hidden)\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "training_data = [([\"NP_PERSON\"], [\"Who is\"]),\n",
    "               ([\"VP\"], [\"What did NP do ?\"])]\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "                            \n",
    "tag_to_ix = {\"Who is\": 0, \"What did NP do ?\": 1}\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "# Create the model\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden  = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores\n",
    "    \n",
    "# set the variables to Train the model\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "#Values Before Training\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "print(\"Values before training\")\n",
    "print(tag_scores)\n",
    "print(\"=======================\")\n",
    "# Train the RNN for 300 iterations\n",
    "for epoch in range(300):\n",
    "    for sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#Values after training        \n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "print(\"Values After training\")\n",
    "print(tag_scores)\n",
    "print(\"=======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.2180 -0.1152\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = prepare_sequence(['VP'], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "\n",
    "print(tag_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
