{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7a75120678>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import CRFTagger\n",
    "import pycrfsuite\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "#Import PyTorch Framework\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = CRFTagger()\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the train set data\n",
    "#train_set  = pd.read_csv('Train_and_test_data/train.txt',sep=' ',names=['word','Brill','tag']).drop('Brill',1)\n",
    "# Format train set data to tuples of a list of lists\n",
    "#train_set = [[tuple(x) for x in train_set.values]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training Code, Keep it commented unless retraining\n",
    "#ct.train(train_set,'model.crf.tagger') \n",
    "\n",
    "#sentence = \"Once upon a time there was a little girl called Cinderella. Cinderella is met a fairy who grants her a wish to be a Princess. Fairy said to Cinderella that she will become a Princess for one night\"\n",
    "sentence = \"Once upon a time there was a little girl called Alice . One day Alice met a Rabbit . The Rabbit said to Alice , follow me Alice I will carry you to my Den in SriLanka\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(passage):\n",
    "    \"\"\"\n",
    "    1.Remove Punctuations and Special characters appear in book chapters\n",
    "    2.Remove Stopwords\n",
    "    3.Return Clean list of words\n",
    "    \"\"\"\n",
    "    exclude_set = set(['“','”',':'])\n",
    "\n",
    "    no_punctuation = [char for char in passage if char not in string.punctuation + \"“”.\"]\n",
    "    no_punctuation = ''.join(no_punctuation)\n",
    "    no_punctuation = [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upon time little girl called Alice One day Alice met Rabbit Rabbit said Alice follow Alice carry Den SriLanka\n"
     ]
    }
   ],
   "source": [
    "#remove stop words and punctuation\n",
    "sentence = process_text(sentence)\n",
    "sentence = ' '.join(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['upon', 'time', 'little', 'girl', 'called', 'Alice', 'One', 'day', 'Alice', 'met', 'Rabbit', 'Rabbit', 'said', 'Alice', 'follow', 'Alice', 'carry', 'Den', 'SriLanka']]\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(sentence)\n",
    "word_list = [[]]\n",
    "word_list.clear()\n",
    "wlist = []\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    for word in sent.split():\n",
    "        wlist.append(word)\n",
    "        j += 1\n",
    "        \n",
    "    wlist.copy\n",
    "    word_list.append(wlist.copy())\n",
    "    wlist.clear()\n",
    "    i += 1\n",
    "    \n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set the model of previously trained data set\n",
    "ct.set_model_file(model_file='model.crf.tagger')\n",
    "word_list = ct.tag_sents(word_list)\n",
    "\n",
    "#test_set = pd.read_csv('Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag'], index_col=False).drop(['Brill','tag'],1)\n",
    "#test_set = test_set.values.tolist()\n",
    "#ct.tag_sents(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_setEval = pd.read_csv('Train_and_test_data/test.txt', sep=' ', names=['words','Brill','tag']).drop('Brill',1)\n",
    "#test_setEval = [[tuple(x) for x in test_setEval.values]]\n",
    "#test the accuracy of the POS\n",
    "#ct.evaluate(test_setEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['upon', 'B-PP'],\n",
       "       ['time', 'B-NP'],\n",
       "       ['little', 'I-NP'],\n",
       "       ['girl', 'I-NP'],\n",
       "       ['called', 'B-VP'],\n",
       "       ['Alice', 'B-NP'],\n",
       "       ['One', 'I-NP'],\n",
       "       ['day', 'I-NP'],\n",
       "       ['Alice', 'I-NP'],\n",
       "       ['met', 'I-NP'],\n",
       "       ['Rabbit', 'I-NP'],\n",
       "       ['Rabbit', 'I-NP'],\n",
       "       ['said', 'B-VP'],\n",
       "       ['Alice', 'B-NP'],\n",
       "       ['follow', 'I-NP'],\n",
       "       ['Alice', 'I-NP'],\n",
       "       ['carry', 'I-NP'],\n",
       "       ['Den', 'I-NP'],\n",
       "       ['SriLanka', 'I-NP']], \n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert list of tuples to a numpy array\n",
    "word_list = np.array(word_list)\n",
    "\n",
    "word_list = np.reshape(word_list, (-1,2))\n",
    "\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find S -> NP VP \n",
    "# Returns if a subject is found\n",
    "# pattern variable = [pat0, pat1, pat2]\n",
    "def findSubject(pattern):\n",
    "    #print('{} -> {} {}'.format(pattern[0], pattern[1], pattern[2]))\n",
    "    if pattern[0] == 'NP' and pattern[1] == 'VP' and pattern[2] == 'NP':\n",
    "        #if true pattern[0] is the subject of a sentence\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Subject Found at index 3\n",
      "Subject -> girl => called Alice\n",
      "--------------------\n",
      "Potential Subject Found at index 11\n",
      "Subject -> Rabbit => said Alice\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#Extract the Subject -> NP VP NP to find the subjects in a sentence\n",
    "#using Markov Chain Model\n",
    "\n",
    "pattern = []\n",
    "subjects = []\n",
    "word_list_length = len(word_list)\n",
    "\n",
    "for i in range(0, len(word_list)):\n",
    "    #print(word_list[i][1].split('-')[1])\n",
    "    try:\n",
    "        if((word_list_length - i == 2)):\n",
    "            break\n",
    "        else:\n",
    "            pattern = [word_list[i][1].split('-')[1], word_list[i+1][1].split('-')[1], word_list[i+2][1].split('-')[1]]\n",
    "            # Call the function findSubject to identify potential subject elements and save them in an array\n",
    "            if findSubject(pattern):\n",
    "                # If returns true consider 1st element as a potential Subject\n",
    "                print('Potential Subject Found at index {}'.format(i))\n",
    "                print('Subject -> {} => {} {}'.format(word_list[i][0], word_list[i+1][0], word_list[i+2][0]))\n",
    "                # Put the phrases in to a sentece and append it to an array.\n",
    "                sub = [word_list[i][0],word_list[i+1][0],word_list[i+2][0]]\n",
    "                subjects.append(' '.join(sub))\n",
    "                print('--------------------')\n",
    "            \n",
    "    except IndexError:\n",
    "        # Break the loop if IndexError occurs\n",
    "        # Fail safe\n",
    "        print(\"Out of Index\")\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS_TAG</th>\n",
       "      <th>Person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>girl</td>\n",
       "      <td>ADJP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>called</td>\n",
       "      <td>VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice</td>\n",
       "      <td>NP</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rabbit</td>\n",
       "      <td>NP</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>said</td>\n",
       "      <td>VP</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alice</td>\n",
       "      <td>NP</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word POS_TAG  Person\n",
       "0    girl    ADJP        \n",
       "1  called      VP        \n",
       "2   Alice      NP  PERSON\n",
       "3  Rabbit      NP  PERSON\n",
       "4    said      VP        \n",
       "5   Alice      NP  PERSON"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find PERSONs in the filtered sentence using spaCy NER and build a DataFrame Object based on the data\n",
    "# DataFrame structure -> Word | POS_TAG | Person\n",
    "\n",
    "#Convert the subjects array to a 2D numpy array\n",
    "subjects = np.array(subjects)\n",
    "subjects = np.reshape(subjects, (-1,1))\n",
    "\n",
    "refferingDataFrame = pd.DataFrame(columns=('Word','POS_TAG','Person'))\n",
    "\n",
    "for i in range(0,len(subjects)):\n",
    "    for j in range(0,len(subjects[i][0].split(\" \"))):\n",
    "        wordBag = subjects[i][0].split(\" \")\n",
    "        # Find POS_TAG \n",
    "        pos = ct.tag([wordBag[j]])\n",
    "        \n",
    "        # Find IF word is Person\n",
    "        person = [ent.label_ for ent in nlp(pos[0][0]).ents]\n",
    "        person = ' '.join(person)\n",
    "        if not person:\n",
    "            person = \"\"\n",
    "            \n",
    "        #Add a row to the DataFrame with the retrived data\n",
    "        refferingDataFrame.loc[len(refferingDataFrame)] = [pos[0][0], pos[0][1].split('-')[1], person]\n",
    "        \n",
    "refferingDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a LUT based on the verb phrase and the Subject phrase of the sentence to make the question\n",
    "# Create a Template to the question to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_Phrase</th>\n",
       "      <th>Question_Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NP PERSON</td>\n",
       "      <td>Who is $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VP</td>\n",
       "      <td>What did $ $ $</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Word_Phrase Question_Phrase\n",
       "0   NP PERSON        Who is $\n",
       "1          VP  What did $ $ $"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LUTdataFrame = pd.read_csv('LUT.csv')\n",
    "LUTdataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did $ $ $ called ?\n",
      "Who is $ Alice ?\n",
      "Who is $ Rabbit ?\n",
      "What did $ $ $ said ?\n",
      "Who is $ Alice ?\n"
     ]
    }
   ],
   "source": [
    "# Based on the data in refferingDataFrame and LUTdataFrame create appropriate qusetions\n",
    "# eg:- if 'Alice' = NP & Person then, \"Who is \"+ Alice(NP)\n",
    "# eg:- if 'called' '= VP then, \"What did \" + Rabbit(NP) said(VP) Alice(NP)\n",
    " \n",
    "for index,row in refferingDataFrame.iterrows():\n",
    "    testCase = (row['POS_TAG'] + \" \" + row['Person']).rstrip()\n",
    "    for lineNo,comparingRow in LUTdataFrame.iterrows():\n",
    "        # Check how many reference points in the LUT table senetence\n",
    "        if(testCase == comparingRow['Word_Phrase']):\n",
    "            #Generate Sentence according the Word_Phrase\n",
    "            Question = comparingRow['Question_Phrase'] + \" \" + row['Word'] + \" ?\"\n",
    "            print(Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
